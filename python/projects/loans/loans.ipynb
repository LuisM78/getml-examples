{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan default prediction\n",
    "\n",
    "In this tutorial you will use the getML Python API in order to\n",
    "\n",
    "1. [Approach a real world dataset](#Data-exploration)\n",
    "2. [Train a single Multirel Model](#Training-a-Multirel-Model)\n",
    "3. [Perform a Hyperparameter optimization](#Hyperparameter-optimization)\n",
    "4. [Access the trained features](#Extracting-Features)\n",
    "\n",
    "The main result is the demonstration of getML's core concepts in a real world problem from the financial sector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This tutorial features a use case from the financial sector. We will use getML in order to predict loan default. A loan is the lending of money to companies or individuals. Banks grant loans in exchange for the promise of repayment. Loan default is defined as the failure to meet this legal obligation, for example when a home buyer fails to make a mortgage payment. It is essential for a bank to estimate the risk it carries when granting loans to potentially non-performing customers.\n",
    "\n",
    "The analysis is based on the [financial](https://relational.fit.cvut.cz/dataset/Financial) dataset from the [the CTU Prague Relational Learning Repository](https://arxiv.org/abs/1511.03086). It contains information about 606 successful and 76 not successful loans and consists of 8 tables:\n",
    "\n",
    "![](dataset_jupyter.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `loan` table contains information about the loans gratend by th bank, such as date of creation, amount and the planned duration of the loan. It also contains our target variable `status` that we will predict in this analysis. Each loan is associated to one account.\n",
    "\n",
    "Each account has both static characteristics (e.g. date of creation, address of the branch) given in `account` and dynamic characteristics (e.g. payments debited or credited, balances) given in relations `order` and `trans`. The table `client` describes characteristics of the account owners. Clients and accounts are related together in relation `disp`. The `card` table describes credit card services which the bank offers to its clients. The table `district` gives publicly available information about the districts an account or client is related to (e.g. the unemployment rate). More information about the dataset can be found [here](https://sorry.vse.cz/~berka/challenge/pkdd1999/berka.htm). \n",
    "\n",
    "For convenience, we will work with CSV exports from the original database. We have provided a zip archive of the files [here](https://static.get.ml/data/loans.zip). First, we will do some data exploration using [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATADIR='../../../data/loans'\n",
    "loan = pd.read_csv(DATADIR + '/loan.csv')\n",
    "account = pd.read_csv(DATADIR + '/account.csv')\n",
    "order = pd.read_csv(DATADIR + '/order.csv')\n",
    "trans = pd.read_csv(DATADIR + '/trans.csv')\n",
    "card = pd.read_csv(DATADIR + '/card.csv')\n",
    "client = pd.read_csv(DATADIR + '/client.csv')\n",
    "disp = pd.read_csv(DATADIR + '/disp.csv')\n",
    "district = pd.read_csv(DATADIR + '/district.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "We will have a closer look at the tables from the financial dataset and simplify the data scheme where possible for later analysis. Such operations are recommended when a join between two tables is possible without any aggregation operations.\n",
    "\n",
    "### Population table\n",
    "\n",
    "The target column is `status` in `loan` table. Each loan, however, is associated with exactly one account. Therefore we left join `account` and `loan` and use the result as population table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = loan.merge(account,\n",
    "                        how='left',\n",
    "                        on='account_id',\n",
    "                        suffixes=('_loan', '_account')\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the resulting population table\n",
    "\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>loan_id</th>\n",
    "      <th>account_id</th>\n",
    "      <th>date_loan</th>\n",
    "      <th>amount</th>\n",
    "      <th>duration</th>\n",
    "      <th>payments</th>\n",
    "      <th>status</th>\n",
    "      <th>district_id</th>\n",
    "      <th>frequency</th>\n",
    "      <th>date_account</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>4959</td>\n",
    "      <td>2</td>\n",
    "      <td>1994-01-05</td>\n",
    "      <td>80952</td>\n",
    "      <td>24</td>\n",
    "      <td>3373.0</td>\n",
    "      <td>A</td>\n",
    "      <td>1</td>\n",
    "      <td>POPLATEK MESICNE</td>\n",
    "      <td>1993-02-26</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>4961</td>\n",
    "      <td>19</td>\n",
    "      <td>1996-04-29</td>\n",
    "      <td>30276</td>\n",
    "      <td>12</td>\n",
    "      <td>2523.0</td>\n",
    "      <td>B</td>\n",
    "      <td>21</td>\n",
    "      <td>POPLATEK MESICNE</td>\n",
    "      <td>1995-04-07</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>4962</td>\n",
    "      <td>25</td>\n",
    "      <td>1997-12-08</td>\n",
    "      <td>30276</td>\n",
    "      <td>12</td>\n",
    "      <td>2523.0</td>\n",
    "      <td>A</td>\n",
    "      <td>68</td>\n",
    "      <td>POPLATEK MESICNE</td>\n",
    "      <td>1996-07-28</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>4967</td>\n",
    "      <td>37</td>\n",
    "      <td>1998-10-14</td>\n",
    "      <td>318480</td>\n",
    "      <td>60</td>\n",
    "      <td>5308.0</td>\n",
    "      <td>D</td>\n",
    "      <td>20</td>\n",
    "      <td>POPLATEK MESICNE</td>\n",
    "      <td>1997-08-18</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>4968</td>\n",
    "      <td>38</td>\n",
    "      <td>1998-04-19</td>\n",
    "      <td>110736</td>\n",
    "      <td>48</td>\n",
    "      <td>2307.0</td>\n",
    "      <td>C</td>\n",
    "      <td>19</td>\n",
    "      <td>POPLATEK TYDNE</td>\n",
    "      <td>1997-08-08</td>\n",
    "    </tr>\n",
    "      <th>...</th>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>677</th>\n",
    "      <td>7294</td>\n",
    "      <td>11327</td>\n",
    "      <td>1998-09-27</td>\n",
    "      <td>39168</td>\n",
    "      <td>24</td>\n",
    "      <td>1632.0</td>\n",
    "      <td>C</td>\n",
    "      <td>7</td>\n",
    "      <td>POPLATEK MESICNE</td>\n",
    "      <td>1997-10-15</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>678</th>\n",
    "      <td>7295</td>\n",
    "      <td>11328</td>\n",
    "      <td>1998-07-18</td>\n",
    "      <td>280440</td>\n",
    "      <td>60</td>\n",
    "      <td>4674.0</td>\n",
    "      <td>C</td>\n",
    "      <td>54</td>\n",
    "      <td>POPLATEK MESICNE</td>\n",
    "      <td>1996-11-05</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>679</th>\n",
    "      <td>7304</td>\n",
    "      <td>11349</td>\n",
    "      <td>1995-10-29</td>\n",
    "      <td>419880</td>\n",
    "      <td>60</td>\n",
    "      <td>6998.0</td>\n",
    "      <td>C</td>\n",
    "      <td>1</td>\n",
    "      <td>POPLATEK TYDNE</td>\n",
    "      <td>1995-05-26</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>680</th>\n",
    "      <td>7305</td>\n",
    "      <td>11359</td>\n",
    "      <td>1996-08-06</td>\n",
    "      <td>54024</td>\n",
    "      <td>12</td>\n",
    "      <td>4502.0</td>\n",
    "      <td>A</td>\n",
    "      <td>61</td>\n",
    "      <td>POPLATEK MESICNE</td>\n",
    "      <td>1994-10-01</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>681</th>\n",
    "      <td>7308</td>\n",
    "      <td>11362</td>\n",
    "      <td>1996-12-27</td>\n",
    "      <td>129408</td>\n",
    "      <td>24</td>\n",
    "      <td>5392.0</td>\n",
    "      <td>A</td>\n",
    "      <td>67</td>\n",
    "      <td>POPLATEK MESICNE</td>\n",
    "      <td>1995-10-14</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `status` column contains 4 different categories: \n",
    "\n",
    "* A stands for contract finished, no problems,\n",
    "* B stands for contract finished, loan not payed,\n",
    "* C stands for running contract, OK so far,\n",
    "* D stands for running contract, client in debt\n",
    "\n",
    "We will consider A and C a successfull loan and B and D a default and transform `status` into a boolean column. Finally, we will randomly spit the data into a training and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "population['status'] = np.where((population['status'] == 'A') | (population['status'] == 'C') , 1, 0)\n",
    "\n",
    "population_training = population[population['account_id'] % 2 == 0]\n",
    "population_validation = population[population['account_id'] % 2 == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peripheral tables\n",
    "\n",
    "We will use all the other tables as peripheral tables. Some of them do not have a time stamps so we will add a dummy one. Also we join `card`, `disp`  and `client` in order to simplify the data scheme. Note that `district` is used twice: It is once joined to the population table and once to the `client` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_client_card = disp.merge(\n",
    "                        client,\n",
    "                        how='left',\n",
    "                        on='client_id'\n",
    "                      ).merge(\n",
    "                        card,\n",
    "                        how='left',\n",
    "                        on='disp_id',\n",
    "                        suffixes=(\"_disp\",\"_card\")\n",
    "                 )\n",
    "disp_client_card['dummy_ts'] = 0\n",
    "district['dummy_ts'] = 0\n",
    "order['dummy_ts'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Multirel Model\n",
    "\n",
    "After having prepared the dataset we can dive into the actual analysis. This is the point where getML sets in with automated feature engineering and model training. First, we will load the data to the getML engine and define the data scheme. Then we will train a [MultirelModel](https://docs.get.ml/latest/api/getml.models.MultirelModel.html) in order to predict the target column `status`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "So far everything happend within pandas. In order to make best use of getML we upload the data into the engine. To this end we create a [DataFrame](https://docs.get.ml/latest/api/getml.engine.DataFrame.html) for each table. During this step we give a [role](https://docs.get.ml/latest/tutorial/data_frames/on_data_frames.html#roles) to each column. That mean we tell the engine how it should interpret the values of each column. Roles are for example \"categorical\" oder \"numerical\" but also include \"target\" and \"join_keys\". After initializing each DataFrame in the Python API we send it to the engine with the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getml\n",
    "getml.engine.set_project('loans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population_training = getml.engine.DataFrame(\n",
    "    \"POPULATION_TRAIN\",\n",
    "    join_keys=[\"account_id\", \"district_id\"],\n",
    "    time_stamps=[\"date_loan\"],\n",
    "    categorical=[\"frequency\"],\n",
    "    numerical=[\"amount\", \"duration\", \"payments\", \"date_account\"],\n",
    "    targets=[\"status\"]\n",
    ")\n",
    "df_population_training.send(population_training);\n",
    "\n",
    "df_population_validation = getml.engine.DataFrame(\n",
    "    \"POPULATION_VALIDATE\",\n",
    "    join_keys=[\"account_id\", \"district_id\"],\n",
    "    time_stamps=[\"date_loan\"],\n",
    "    categorical=[\"frequency\"],\n",
    "    numerical=[\"amount\", \"duration\", \"payments\", \"date_account\"],\n",
    "    targets=[\"status\"]\n",
    ")\n",
    "df_population_validation.send(population_validation);\n",
    "\n",
    "\n",
    "df_order = getml.engine.DataFrame(\n",
    "    name=\"ORDER\",\n",
    "    numerical=[\"amount\"],\n",
    "    categorical=[\"bank_to\", \"k_symbol\", \"account_to\"],\n",
    "    join_keys=[\"account_id\"],\n",
    "    time_stamps=[\"dummy_ts\"]\n",
    ")\n",
    "df_order.send(order);\n",
    "\n",
    "df_trans = getml.engine.DataFrame(\n",
    "    name=\"TRANS\",\n",
    "    numerical=[\"amount\", \"balance\"],\n",
    "    categorical=[\"type\", \"k_symbol\", \"bank\", \"account\", \"operation\"],\n",
    "    join_keys=[\"account_id\"],\n",
    "    time_stamps=[\"date\"]\n",
    ")\n",
    "df_trans.send(trans)\n",
    "\n",
    "df_disp_client_card = getml.engine.DataFrame(\n",
    "    name=\"DISP_CLIENT_CARD\",\n",
    "    numerical=['birth_date', 'issued'],\n",
    "    categorical=['type_disp', 'type_card', 'gender'],\n",
    "    join_keys=[\"account_id\", \"district_id\"],\n",
    "    time_stamps=[\"dummy_ts\"]\n",
    ")\n",
    "df_disp_client_card.send(disp_client_card)\n",
    "\n",
    "df_district = getml.engine.DataFrame(\n",
    "    name=\"DISTRICT\",\n",
    "    categorical = [\"A2\", \"A3\"],\n",
    "    numerical = [\"A4\", \"A5\", \"A6\", \"A7\", \"A8\", \"A9\", \"A10\", \"A11\", \"A12\", \"A13\", \"A14\", \"A15\", \"A16\"],\n",
    "    join_keys=[\"district_id\"],\n",
    "    time_stamps=[\"dummy_ts\"]\n",
    ")\n",
    "df_district.send(district);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the data model. This is the slighly modified scheme we introduced in the first section. The MultirelModel expectes a [Placeholder](https://docs.get.ml/latest/api/getml.models.Placeholder.html#placeholder) for each table. These Placeholders are then joined together in order to hand the data model over to the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "\n",
    "population_placeholder = df_population_training.to_placeholder()\n",
    "order_placeholder = df_order.to_placeholder()\n",
    "trans_placeholder = df_trans.to_placeholder()\n",
    "disp_client_card_placeholder = df_disp_client_card.to_placeholder()\n",
    "district_placeholder = df_district.to_placeholder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data model\n",
    "\n",
    "population_placeholder.join(order_placeholder,\n",
    "                            join_key=\"account_id\",\n",
    "                            time_stamp=\"date_loan\",\n",
    "                            other_time_stamp=\"dummy_ts\")\n",
    "population_placeholder.join(trans_placeholder,\n",
    "                            join_key=\"account_id\", \n",
    "                            time_stamp=\"date_loan\",\n",
    "                            other_time_stamp=\"date\")\n",
    "population_placeholder.join(disp_client_card_placeholder,\n",
    "                            join_key=\"account_id\", \n",
    "                            time_stamp=\"date_loan\",\n",
    "                            other_time_stamp=\"dummy_ts\")\n",
    "population_placeholder.join(district_placeholder,\n",
    "                            join_key=\"district_id\", \n",
    "                            time_stamp=\"date_loan\",\n",
    "                            other_time_stamp=\"dummy_ts\")\n",
    "disp_client_card_placeholder.join(district_placeholder,\n",
    "                            join_key=\"district_id\",\n",
    "                            time_stamp='dummy_ts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last missing piece before the training can start is to tell the engine about the unit for each columns. Columns will the same unit will be compared by the MultirelModel in order to create features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define units\n",
    "\n",
    "units = dict(\n",
    "    birth_date = 'timestamp',\n",
    "    issued = 'timestamp',\n",
    "    date_loan = 'timestamp',\n",
    "    date_account = 'timestamp',\n",
    "    amount = 'money',\n",
    "    balance = 'money'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now, we are ready to train a MultirelModel. We will start with the defaut settings and take care of the hyperparameter optimization later on. Input to the model are a feature selector and a predictor. We will use XGBoost for both in this tutorial. We also need to provied the placeholders defined above as the units dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selector and predictor\n",
    "\n",
    "feature_selector = getml.predictors.XGBoostClassifier(\n",
    "    reg_lambda=500\n",
    ")\n",
    "\n",
    "predictor = getml.predictors.XGBoostClassifier(\n",
    "    reg_lambda=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getml.models.MultirelModel(\n",
    "    aggregation=[\n",
    "        getml.aggregations.Avg,\n",
    "        getml.aggregations.Count,\n",
    "        getml.aggregations.CountDistinct,\n",
    "        getml.aggregations.CountMinusCountDistinct,\n",
    "        getml.aggregations.Max,\n",
    "        getml.aggregations.Median,\n",
    "        getml.aggregations.Min,\n",
    "        getml.aggregations.Sum,\n",
    "        getml.aggregations.Var\n",
    "    ],\n",
    "    num_features=30,\n",
    "    population=population_placeholder,\n",
    "    peripheral=[order_placeholder, trans_placeholder, disp_client_card_placeholder, district_placeholder],\n",
    "    loss_function=getml.loss_functions.CrossEntropyLoss(),\n",
    "    feature_selector=feature_selector,\n",
    "    predictor=predictor,\n",
    "    units=units\n",
    ").send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(\n",
    "    population_table=df_population_training,\n",
    "    peripheral_tables=[df_order, df_trans, df_disp_client_card, df_district]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training time of the model is well below one minute. Let's look at how well the model performs on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample = model.score(\n",
    "        population_table=df_population_training,\n",
    "        peripheral_tables=[df_order, df_trans, df_disp_client_card, df_district]\n",
    ")\n",
    "\n",
    "out_of_sample = model.score(\n",
    "        population_table=df_population_validation,\n",
    "        peripheral_tables=[df_order, df_trans, df_disp_client_card, df_district]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The in sample accuracy is roughly 98%, the out of sample accuracy is just above 90%. This is a promising result but the model is still overfitting the training data so we need to adjust the regularization. The AUC value are 0.97 and 0.77, respecively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "The much higher in sample accuracy is a strong hint that the model is overfitting the data. We will perform a hyperparamter optimization to improve the out of sample accuracy. We will do this using a Latin Hypercube search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = dict()\n",
    "\n",
    "param_space[\"grid_factor\"] = [1.0, 16.0]\n",
    "param_space[\"max_length\"] = [1, 10]\n",
    "param_space[\"num_features\"] = [10, 100]\n",
    "param_space[\"regularization\"] = [0.0, 0.01]\n",
    "param_space[\"share_aggregations\"] = [0.01, 0.3]\n",
    "param_space[\"share_selected_features\"] = [0.1, 1.0]\n",
    "param_space[\"shrinkage\"] = [0.01, 0.4]\n",
    "\n",
    "# Hyperparameters that relate to the predictor\n",
    "param_space[\"predictor__n_estimators\"] = [100, 400]\n",
    "param_space[\"predictor__max_depth\"] = [3, 15]\n",
    "param_space[\"predictor__reg_lambda\"] = [0.0, 1000.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_search = getml.hyperopt.LatinHypercubeSearch(\n",
    "    model=model,\n",
    "    param_space=param_space,\n",
    "    n_iter=30\n",
    ")\n",
    "\n",
    "latin_search.fit(\n",
    "  population_table_training=df_population_training,\n",
    "  population_table_validation=df_population_validation,\n",
    "  peripheral_tables=[df_order, df_trans, df_disp_client_card, df_district]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model\n",
    "scores = latin_search.get_scores()\n",
    "best_model_name = max(scores, key=lambda key: scores[key]['auc_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The out of sample accuracy is now at 92%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features\n",
    "\n",
    "So far we have trained a MultirelModel and done a hyperparameter optimization. But what did actually happen behined the scenes? In order to gain insight into the features the Multirel Model has construced we will look at the in SQL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = getml.models.load_model(best_model_name)\n",
    "best_model.to_sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature with the highest importance is feature 1. The corresponding SQL code reads\n",
    "\n",
    "```sql\n",
    "CREATE TABLE FEATURE_1 AS\n",
    "SELECT COUNT( * ) - COUNT( DISTINCT t1.date_loan - t2.date ) AS feature_1,\n",
    "       t1.account_id,\n",
    "       t1.date_loan\n",
    "FROM (\n",
    "     SELECT *,\n",
    "            ROW_NUMBER() OVER ( ORDER BY account_id, date_loan ASC ) AS rownum\n",
    "     FROM POPULATION_TRAIN\n",
    ") t1\n",
    "LEFT JOIN TRANS t2\n",
    "ON t1.account_id = t2.account_id\n",
    "WHERE (\n",
    "   ( t2.balance > 2897.401575 AND t2.account NOT IN ( '0.0', '14627766.0' ) AND t2.account IN (\n",
    "       '29053187.0', '9528249.0', '34606760.0', '70089435.0', '9956103.0', '1786172.0', '73765313.0',\n",
    "       '69850820.0', '25744539.0', '13214141.0', '49724513.0', '57185012.0', '85668760.0', '10182565.0',\n",
    "       '88636147.0', '95071893.0', '88992631.0', '94599204.0', '56820648.0', '62845337.0', '57953965.0',\n",
    "       '75713777.0', '61734002.0', '89302223.0', '8741110.0', '69871258.0', '27539909.0', '32746297.0',\n",
    "       '63132923.0', '34098577.0', '51522499.0', '59294611.0', '60735916.0', '85381936.0', '91262608.0',\n",
    "       '23212576.0', '69885569.0', '63854801.0', '40573519.0', '53127735.0', '64414620.0', '69220012.0',\n",
    "       '78708076.0', '2707452.0', '42320823.0', '11192852.0', '40423691.0', '59493338.0', '53895962.0',\n",
    "       '61556398.0', '4807711.0', '72567075.0', '11309677.0', '71522050.0', '67477196.0', '65871029.0',\n",
    "       '80826598.0', '40799850.0' ) )\n",
    "OR ( t2.balance > 2897.401575 AND t2.account IN ( '0.0', '14627766.0' )\n",
    "    AND t1.date_loan - t2.date > 135.238095 AND t1.payments > 5090.500000 )\n",
    "OR ( t2.balance > 2897.401575 AND t2.account IN ( '0.0', '14627766.0' )\n",
    "    AND t1.date_loan - t2.date <= 135.238095 )\n",
    "OR ( t2.balance <= 2897.401575 )\n",
    ") AND t2.date <= t1.date_loan\n",
    "GROUP BY t1.rownum,\n",
    "         t1.account_id,\n",
    "         t1.date_loan;\n",
    "```\n",
    "\n",
    "The feature is created using the peripheral table `trans`. The SQL code is quite cumbersome to read. This is because we went for maximum performance in this tutorial. In a later analysis you will see how to regularize the MultirelModel in order to achieve more interpretable features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We are able to predict loan default in the example dataset with an accuracy of over 90%. With this result getML is in the top 1% of the [algorithms that worked on this problem](https://relational.fit.cvut.cz/dataset/Financial). The training time for the initial model was less than one minute and the hyperparameter optimization took less than one hour. Together with the data preparation this project can easily be completed within one day.\n",
    "\n",
    "You can use this tutorial as starting point for your own analysis or head over the other tutorials and the user guide if you want to learn more about the functionalty getML offers. Please [contact us](https://get.ml/contact/lets-talk) with your feedback about this tutorial or general inquiries. We also offer proof-of-concept project if you need help getting started with getML. A Jupyter notebook with this tutorial is available on GitHub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
